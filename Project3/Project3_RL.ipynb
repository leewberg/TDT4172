{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb6euaGTtpwB"
      },
      "source": [
        "# **Project 3 - Reinforcement Learning**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ECxNoBGPkGr"
      },
      "source": [
        "## 🌞 Mission: Find a secret Location in Trondheim where SkyNets servers are located"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDMCdxyPkGs"
      },
      "source": [
        "📊 **High-level goal:**\n",
        "\n",
        "Train an agent (the “learner”) to interact with given environment and learn from it, so it can maximize its cumulative rewards over time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **Environment:**\n",
        "     *   The environment **is already created** and set up for you with the help of\n",
        "\"gymnasium\" library.\n",
        "     *   The agent is interacting with a simulated environment, which has states, accepts actions, and returns rewards. The environment resets at the beginning of each episode, and the agent takes actions within it, receiving feedback in the form of rewards.\n",
        "*   **Q-table:**\n",
        "     *   This table stores information about the \"quality\" of actions in different states.\n",
        "     *   The goal of the agent is to update the Q-table over time to estimate the value of taking different actions in various states.\n",
        "     *   You will initialize and learn Q-table in this project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URR74x6gPkGs"
      },
      "source": [
        "## 📮 Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtDaL3aL9emb"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v3 as iio\n",
        "import random\n",
        "import os\n",
        "\n",
        "# note: make sure to install necessary libraries like imageio, gymnasium etc.\n",
        "# !pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq1P8fu_itVF"
      },
      "outputs": [],
      "source": [
        "# import gymnasium stuff\n",
        "import gymnasium as gym\n",
        "from gymnasium import error, spaces, utils\n",
        "from gymnasium.utils import seeding\n",
        "from gymnasium.spaces import Discrete, MultiDiscrete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UCkLhnL5hik"
      },
      "source": [
        "### Utility functions are provided for you:\n",
        "\n",
        "\n",
        "*   one_hot_decode\n",
        "*   one_hot_encode\n",
        "*   make_2d\n",
        "*   make_3d\n",
        "*   read_png_file\n",
        "*   is_valid_idx\n",
        "*   flatten_observation\n",
        "*   is_valid_idx\n",
        "*   flatten_observation\n",
        "*   make_2d_observation\n",
        "*   swap_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6bisXF65lHl"
      },
      "outputs": [],
      "source": [
        "def one_hot_decode(label_3D):\n",
        "  \"\"\"\n",
        "  Converts back from one-hot encoded format (label_3D)\n",
        "  to original class labels for each pixel (2D)\n",
        "  \"\"\"\n",
        "  return np.argmax(label_3D, axis=-1)\n",
        "\n",
        "def one_hot_encode(label_2D, label_values):\n",
        "    \"\"\"\n",
        "    Converts a segmentation image label array (label_2D) to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label_2D: The 2D array segmentation image label\n",
        "        label_values: an RGB array of classes (num_classes, 3)\n",
        "\n",
        "    # Returns\n",
        "        A 3D array with the same width and height as the input,\n",
        "        but with a depth size of num_classes (for each pixel)\n",
        "\n",
        "        Each class gets its own encoding for every pixel in the image.\n",
        "        Encodings are then stacked along new axis which is number of classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label_2D, colour)\n",
        "        class_map = np.all(equality, axis=-1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "\n",
        "def make_2d(map_3d=None, palette_array=None):\n",
        "    \"\"\"\n",
        "    Converts RGB images to images with number of class as value at [i,j]\n",
        "\n",
        "    map_3d: 3D RGB image (H, W, n_ch = 3)\n",
        "    palette_array: an RGB array of classes (num_classes, 3)\n",
        "\n",
        "    Result: 2D structure where each \"pixel\" in the image\n",
        "    has a class number instead of RGB color\n",
        "    \"\"\"\n",
        "    # here we make 3D representation, 3rd dimension has one-hot encoded classes\n",
        "    replaced_image_onehot = one_hot_encode(\n",
        "        map_3d.astype(np.uint8), palette_array) # size (H, W, num_classes)\n",
        "    # we create 2D representation, where every pixel has the value of its class\n",
        "    return one_hot_decode(replaced_image_onehot) # size (H, W) - 2D\n",
        "\n",
        "def make_3d(map_2d=None, palette_array=None):\n",
        "    \"\"\"\n",
        "    Convert a 2d img to 3d\n",
        "    Convert a 2D image where each pixel represents a class label\n",
        "    into a 3D image where each class label is represented\n",
        "    by its corresponding RGB color from the palette_array.\n",
        "    Creates colorized 3D image from the original 2D class map\n",
        "\n",
        "    map_2d: array of class labels, size (H, W)\n",
        "    palette_array: an RGB array of classes (num_classes, 3)\n",
        "\n",
        "    Result: RGB image (H, W, 3)\n",
        "    \"\"\"\n",
        "    return palette_array[map_2d.astype(np.uint8)]\n",
        "\n",
        "\n",
        "def read_png_file(image_path, printDebug = True):\n",
        "    \"\"\"\n",
        "    read a png file and returns it as a 3D numpy array\n",
        "    \"\"\"\n",
        "    original_image_matrix = iio.imread(image_path)\n",
        "\n",
        "    if original_image_matrix.ndim > 2 and original_image_matrix.shape[-1] > 3:\n",
        "        if printDebug:\n",
        "            print(f'image has more than 3 channels, only first 3 channels are used there are {original_image_matrix.shape} channels')\n",
        "        original_image_matrix = original_image_matrix[:, :, :3]\n",
        "    return original_image_matrix\n",
        "\n",
        "def is_valid_idx(image_2D, selected_idx):\n",
        "    \"\"\"\n",
        "    Checks if a given index is within the valid range of a 2D array\n",
        "\n",
        "    if selected_idx is valid in image_2D\n",
        "    selected_idx = [i,j]\n",
        "    image_2D = [h,w]\n",
        "      \"\"\"\n",
        "    if selected_idx[0] < 0 or selected_idx[1] < 0:\n",
        "        return False\n",
        "    if selected_idx[0] < image_2D.shape[0]:\n",
        "        if selected_idx[1] < image_2D.shape[1]:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def flatten_observation(observation_arr):\n",
        "    \"\"\"\n",
        "    Converts/flattens a multi-dimensional array into a 1D array\n",
        "    \"\"\"\n",
        "    return observation_arr.flatten\n",
        "\n",
        "\n",
        "def make_2d_observation(observation_flattened, observation_arr_shape):\n",
        "    \"\"\"\n",
        "    Converts flattened array into shape observation_arr_shape\n",
        "    \"\"\"\n",
        "    return observation_flattened.reshape(observation_arr_shape.shape)\n",
        "\n",
        "def swap_values(arr, value1, value2):\n",
        "    \"\"\"\n",
        "    Swaps occurencies of two specific values (value1 and value2)\n",
        "    within a 2D NumPy array (arr).\n",
        "\n",
        "    arr: A 2D NumPy array where we want to swap the values\n",
        "    value1: the first value that we want to swap with value2\n",
        "    value2: the second value that we want to swap with value1\n",
        "\n",
        "    Returns: 2D array with swapped values\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the values are integers\n",
        "    value1, value2 = int(value1), int(value2)\n",
        "\n",
        "    # Step 1: Find the first position of value1 and change it to value2\n",
        "    pos_value1 = np.argwhere(arr == value1) # returns 2D array\n",
        "    if pos_value1.size > 0:\n",
        "        arr[pos_value1[0][0], pos_value1[0][1]] = value2\n",
        "\n",
        "    # Step 2: Get all positions of value2\n",
        "    pos_value2 = np.argwhere(arr == value2) # returns 2D array\n",
        "\n",
        "    # Step 3: Randomly select a position of value2 that is not the original value1's position\n",
        "    if len(pos_value2) > 1:\n",
        "        # filter out the position of former value1 (pos_value1) which now also contains value2\n",
        "        pos_value2 = pos_value2[np.all(pos_value2 != pos_value1[0], axis=1)]\n",
        "        if pos_value2.size > 0:\n",
        "            random_pos_value2 = pos_value2[np.random.choice(len(pos_value2))]\n",
        "            arr[random_pos_value2[0], random_pos_value2[1]] = value1\n",
        "\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I95zkEvIPkGt"
      },
      "source": [
        "### Environment class with necessary functions - provided for you:\n",
        "\n",
        "\n",
        "*   constructor __init__\n",
        "*   step\n",
        "*   reset\n",
        "*   get_1d_state\n",
        "*   render\n",
        "*   close\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6TWGiuXth-H"
      },
      "outputs": [],
      "source": [
        "class TrondheimEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, conf):\n",
        "        \"\"\"\n",
        "        Every environment should be derived from gym.Env and at least contain\n",
        "        variables observation_space and action_space specifying the type of possible\n",
        "        observations and actions using spaces.Box or spaces.Discrete.\n",
        "\n",
        "        Example:\n",
        "        >>> EnvTest = TrondheimEnv()\n",
        "        >>> EnvTest.observation_space=spaces.Box(low=-1, high=1, shape=(3,4))\n",
        "        >>> EnvTest.action_space=spaces.Discrete(2)\n",
        "        \"\"\"\n",
        "\n",
        "        self.manual_mode = True  # Flag to control printouts\n",
        "\n",
        "        # action definition:\n",
        "        number_action = 4\n",
        "        self.action_space = Discrete(number_action)\n",
        "\n",
        "        self.env_image_path = conf['env_image_path']\n",
        "        self.number_removable_locations = conf['number_removable_locations']\n",
        "\n",
        "        self.pallete = np.array([[0, 0, 255],    # Blue:0:Water\n",
        "                            [255, 255, 0],  # Yellow:1:Start\n",
        "                            [0, 255, 0],    # Green:2:Land\n",
        "                            [255, 0, 0],    # Red:3:BusLane\n",
        "                            [128, 0, 128],  # Purple:4:BusStop\n",
        "                            [0,   0,   0],  # Black:5:Secret Location\n",
        "                            [128, 128, 0]], # Olive:6:Final Location\n",
        "                           dtype=np.uint8)\n",
        "\n",
        "        self.neighbours = np.array([[-1,0],[0,-1],[1,0],[0,1]]) # up, left, down, right\n",
        "        self.not_allowed_area = [0] # water\n",
        "        self.normal_land_class = 2 # land\n",
        "        self.neutral_area = [self.normal_land_class,3] # land and BusLane\n",
        "        self.start_class = 1 # start\n",
        "        self.busstop_class = 4 # bus stop\n",
        "        self.secret_location_class = 5 # secret location\n",
        "        self.final_location_class = 6 # final location (2)\n",
        "\n",
        "        # does agent die when enter not_allowed_area\n",
        "        self.dead_allowed = conf['dead_allowed']\n",
        "        # does shops closed after the first purchase\n",
        "        self.remove_after_location_found = conf['remove_after_location_found']\n",
        "        # should we start at different place each time\n",
        "        self.start_random = conf['start_random']\n",
        "\n",
        "        # read RGB image into a 3D numpy array\n",
        "        img_3d = read_png_file(self.env_image_path, printDebug = False)\n",
        "\n",
        "        # convert RGB image into 2D semantic class representation\n",
        "        img_2d = make_2d(map_3d=img_3d,\n",
        "                         palette_array=self.pallete)\n",
        "\n",
        "        # the current state of the environment\n",
        "        self.observation = img_2d # initial observation - 2D semantic class\n",
        "        # copy of the environment's initial state before any actions have been taken\n",
        "        self.initial_observation = img_2d.copy()\n",
        "\n",
        "        max_observation_value = np.max(img_2d)\n",
        "        # assert isinstance(max_observation_value, int)\n",
        "        # can be used, not needed for this implementation\n",
        "        # observation space is a gym object (MultiDiscrete), like numpy array. It can be sampled\n",
        "        observation_space = np.full_like(img_2d,\n",
        "                                         fill_value=max_observation_value)\n",
        "        self.observation_space = MultiDiscrete(observation_space)\n",
        "\n",
        "        # rewards\n",
        "        self.reward = 0\n",
        "\n",
        "        # done\n",
        "        self.done = False\n",
        "\n",
        "        # terminated\n",
        "        self.terminated = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        This method is the primary interface between environment and agent.\n",
        "\n",
        "        Parameters:\n",
        "            action: int\n",
        "                    the index of the respective action (if action space is discrete)\n",
        "\n",
        "        Returns:\n",
        "            output: (array, float, bool, info)\n",
        "                    information provided by the environment about its current state:\n",
        "                    (observation, reward, done)\n",
        "\n",
        "                    self.observation: The updated state of the environment (2D)\n",
        "                    reward: Reward agent received for the action in this step (Int)\n",
        "                    terminated: indicate if the episode has ended (Bool)\n",
        "                    info: Additional information (not used currently)\n",
        "\n",
        "        \"\"\"\n",
        "        obs = self.observation # 2D array of semantic class image representation\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        info = None\n",
        "\n",
        "        # just start and see where it goes\n",
        "        start_idx = np.argwhere(obs == self.start_class)[0]\n",
        "        # action is: 0:up, 1:left, 2:down, 3:right\n",
        "        next_idx = start_idx + self.neighbours[action]\n",
        "        reward = self.reward\n",
        "        if self.manual_mode:\n",
        "          print('Next step towards: ')\n",
        "\n",
        "        if not is_valid_idx(image_2D=obs, selected_idx=next_idx):\n",
        "            next_idx = start_idx\n",
        "            if self.manual_mode:\n",
        "                print('Hey there, you cannot escape outside of the environment')\n",
        "\n",
        "        # now decide what happens next\n",
        "\n",
        "        if obs[tuple(next_idx)] == self.normal_land_class:\n",
        "            if self.manual_mode:\n",
        "                print('Green Land class area')\n",
        "\n",
        "        # check if it is a non allowed area\n",
        "        if obs[tuple(next_idx)] in self.not_allowed_area:\n",
        "            if self.manual_mode:\n",
        "                print('The move is not allowed')\n",
        "            if self.dead_allowed:\n",
        "                # penalize the agent if allowed to die. Terminate\n",
        "                reward = -1000\n",
        "                terminated = True\n",
        "            else:\n",
        "                # if NOT allowed to die, reset the next position to current one - prevent the move\n",
        "                next_idx = start_idx\n",
        "\n",
        "\n",
        "        # if the player is in bus stup\n",
        "        elif obs[tuple(next_idx)] == self.busstop_class:\n",
        "            if self.manual_mode:\n",
        "                print('A bus stop')\n",
        "\n",
        "            # get the bus stop that is not the next step\n",
        "            bus_stop_indices = np.argwhere(obs == self.busstop_class)\n",
        "            # flip the next state to the other side of the stop\n",
        "            if np.array_equal(next_idx, bus_stop_indices[0]):\n",
        "                # teleport the agent from bus stop 0 to bus stop 1\n",
        "                next_idx = bus_stop_indices[1]\n",
        "            else:\n",
        "                # teleport the agent from bus stop 1 to bus stop 0\n",
        "                next_idx = bus_stop_indices[0]\n",
        "\n",
        "        # if it is Secret Location\n",
        "        elif obs[tuple(next_idx)] == self.secret_location_class:\n",
        "            if self.manual_mode:\n",
        "                print('Central Source Secret Location')\n",
        "            reward = 300\n",
        "            # close the shop (remove from the map)\n",
        "            if self.remove_after_location_found:\n",
        "                self.initial_observation[tuple(next_idx)] = self.normal_land_class\n",
        "                self.number_removed_shop += 1\n",
        "                reward = 9000\n",
        "\n",
        "        # and the Final Location\n",
        "        elif obs[tuple(next_idx)] == self.final_location_class:\n",
        "            if self.manual_mode:\n",
        "                print('Final Location - well done!')\n",
        "                print('You have discovered the Final Location of AI center in Trondheim')\n",
        "            reward = 200\n",
        "\n",
        "\n",
        "        # now we need to change the observation space\n",
        "        # i.e. update the environment’s state to reflect the agent's movement\n",
        "\n",
        "        # if agent’s current position was initial start position\n",
        "        if self.initial_observation[tuple(start_idx)] == self.start_class:\n",
        "            # if true, update observation - mark the start position as neutral area\n",
        "            self.observation[tuple(start_idx)] = self.normal_land_class\n",
        "        else:\n",
        "            # if current position not the initial start one\n",
        "            # restores position in self.observation to its original value from self.initial_observation\n",
        "            # re-color the observation as where you were if not start position\n",
        "            self.observation[tuple(start_idx)] = self.initial_observation[tuple(start_idx)]\n",
        "\n",
        "        # sets the new position of the agent as the next start position\n",
        "        self.observation[tuple(next_idx)] = self.start_class\n",
        "\n",
        "        return self.observation, reward, terminated, info\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        This method resets the environment to its initial values.\n",
        "\n",
        "        Returns:\n",
        "            observation:    array\n",
        "                            the initial state of the environment\n",
        "        \"\"\"\n",
        "        self.number_removed_shop = 0\n",
        "\n",
        "        img_3d = read_png_file(self.env_image_path, printDebug = False)\n",
        "        img_2d = make_2d(map_3d=img_3d,\n",
        "                         palette_array=self.pallete)\n",
        "\n",
        "        if self.start_random:\n",
        "          # we need to move the start to another place\n",
        "          img_2d = swap_values(arr=img_2d,\n",
        "                               value1=1,\n",
        "                               value2=2)\n",
        "\n",
        "        self.observation = img_2d\n",
        "        self.initial_observation = img_2d.copy()\n",
        "\n",
        "        max_observation_value = np.max(img_2d)\n",
        "        observation_space = np.full_like(img_2d,\n",
        "                                         fill_value=max_observation_value)\n",
        "        self.observation_space = MultiDiscrete(observation_space)\n",
        "\n",
        "        # rewards\n",
        "        self.reward = 0\n",
        "\n",
        "        # done\n",
        "        self.done = False\n",
        "\n",
        "        # terminated\n",
        "        self.terminated = False\n",
        "\n",
        "        return self.observation\n",
        "\n",
        "\n",
        "    def get_1d_state(self):\n",
        "        \"\"\"\n",
        "        This function returns state as 1d observation\n",
        "        \"\"\"\n",
        "        max_x = self.observation.shape[0]\n",
        "        max_y = self.observation.shape[1]\n",
        "\n",
        "        start_idx = np.argwhere(\n",
        "            self.observation == self.start_class)[0]\n",
        "        state_number = (\n",
        "            start_idx[0] * max_x) + start_idx[1]\n",
        "        state_number = (self.number_removed_shop * max_x * max_y) + state_number\n",
        "        return state_number\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        \"\"\"\n",
        "        This methods provides the option to render the environment's behavior to a\n",
        "        window which should be readable to the human eye if mode is set to 'human'.\n",
        "        \"\"\"\n",
        "        render_ready_arr = make_3d(map_2d=self.observation,\n",
        "                                   palette_array=self.pallete)\n",
        "        plt.imshow(render_ready_arr)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        return render_ready_arr\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        This method provides the user with the option to perform necessary\n",
        "        cleanup.\n",
        "        \"\"\"\n",
        "        img_3d = read_png_file(self.env_image_path)\n",
        "        img_2d = make_2d(map_3d=img_3d,\n",
        "                         palette_array=self.pallete)\n",
        "        self.observation = img_2d\n",
        "        self.initial_observation = img_2d.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYWhRI2VygG9"
      },
      "outputs": [],
      "source": [
        "# let's make sure to create subfolder for our input images, if not already there\n",
        "input_folder = 'input_images'\n",
        "if not os.path.exists(input_folder):\n",
        "    os.makedirs(input_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LESqxWq8PkGu"
      },
      "source": [
        "# Configuring and loading the environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPy3pcnbPkGu"
      },
      "source": [
        "## 🔎📉  Let's first visualize our simple toy environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEmdFlf2PkGu"
      },
      "outputs": [],
      "source": [
        "# This is how you create a configuration for the environment\n",
        "# Note: make sure that folder input_images is created\n",
        "# Note 2: make sure you copy necessary input images there\n",
        "\n",
        "env_config = {\n",
        "    'env_image_path':'input_images/image_simple_1.png',\n",
        "    'dead_allowed':False,\n",
        "    'remove_after_location_found':False,\n",
        "    'start_random':False,\n",
        "    'number_removable_locations':0,\n",
        "}\n",
        "\n",
        "\n",
        "if not os.path.exists(env_config['env_image_path']):\n",
        "  # make sure you have the folder and the image\n",
        "  print(f\"Error: The image path {env_config['env_image_path']} does not exist.\")\n",
        "else:\n",
        "  # if everything fine, we create the environment\n",
        "  tmp_env = TrondheimEnv(conf=env_config)\n",
        "  print(f\"Environment successfully created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_clXneCvPkGu"
      },
      "outputs": [],
      "source": [
        "# visualize the environment. Yellow is where we are, Black is where we want to go\n",
        "tmp_env.reset()\n",
        "tmp_env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVfyvoCRPkGv"
      },
      "source": [
        "Let's see how actions work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22Lbe8OPkGv"
      },
      "outputs": [],
      "source": [
        "action_dict = {\n",
        "    'up':0,\n",
        "    'left':1,\n",
        "    'down':2,\n",
        "    'right':3,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wUXsfWIPkGv"
      },
      "outputs": [],
      "source": [
        "# select your action here\n",
        "# you are encouraged to play further with this\n",
        "my_action = 'left'\n",
        "\n",
        "# now take a step through the environment by taking this action\n",
        "obs, rew, term, info = tmp_env.step(action_dict[my_action])\n",
        "print(f\"Reward for action {my_action} is: {rew}\")\n",
        "tmp_env.render()\n",
        "\n",
        "print(f\"Observation space looks like this:\\n{obs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BKNX9F4PkGv"
      },
      "source": [
        "# 🧠 **Your Assignments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpfZYKmOPkGv"
      },
      "source": [
        "# 1️⃣ Assignment 1: Q-learning with simple toy environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvG3QGzTPkGv"
      },
      "source": [
        "## 🐣 Let's learn how to do this in a simple environment\n",
        "\n",
        "\n",
        "*   Task 1: Initialize Q table\n",
        "*   Task 2: Write necessary helper functions for Q-learning\n",
        "*   Task 3: Play with Hyperparameters\n",
        "*   Task 4: Train the agent using Q-learning\n",
        "\n",
        "**Note**: Task 2, Task 3 and Task 4: you will use the same code you implemented for these in the Assignment 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdWAfr8fPkGv"
      },
      "outputs": [],
      "source": [
        "# Environment is set up for you here\n",
        "env_config = {\n",
        "    'env_image_path':'input_images/image_simple_1.png',\n",
        "    'dead_allowed':False,\n",
        "    'remove_after_location_found':False,\n",
        "    'start_random':False,\n",
        "    'number_removable_locations':0,\n",
        "}\n",
        "\n",
        "if not os.path.exists(env_config['env_image_path']):\n",
        "  # make sure you have the folder and the image\n",
        "  print(f\"Error: The image path {env_config['env_image_path']} does not exist.\")\n",
        "else:\n",
        "  # if everything fine, we create the environment\n",
        "  tmp_env = TrondheimEnv(conf=env_config)\n",
        "  print(f\"Environment successfully created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PT6w7C-PkGw"
      },
      "source": [
        "### Task 1: Initialize Q table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guc-6hxaPkGw"
      },
      "outputs": [],
      "source": [
        "# Task 1: Initialize Q table\n",
        "\"\"\"\n",
        "TODO: given the environment, initialize the Q table\n",
        "\"\"\"\n",
        "\n",
        "# Tip: pay attention to the dimensions of Q table\n",
        "# Tip: how many states does the table have, how many actions\n",
        "\n",
        "print(f\"Observation space dimensions:  {tmp_env.observation_space.shape}\")\n",
        "print(f\"Number of possible actions: {tmp_env.action_space.n}\")\n",
        "\n",
        "# YOUR CODE GOES HERE\n",
        "Q_test = None # you need to initialize this variable\n",
        "#print(Q_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg715k80PkGw"
      },
      "outputs": [],
      "source": [
        "# take a look into initial Q table (randomized one)\n",
        "print(f\"Q table has this shape: {Q_test.shape}\\n\")\n",
        "\n",
        "# you may want to print Q table, or parts of it, to get the vibe\n",
        "print(Q_test[:10])\n",
        "print(f\"\\n\")\n",
        "#print(Q_test[135:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXiBz_haPkGw"
      },
      "source": [
        "### Task 2: Write necessary helper functions for Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ySxhzOxPkGw"
      },
      "outputs": [],
      "source": [
        "# Task 2: Write necessary helper functions for Q-learning\n",
        "\"\"\"\n",
        "TODO: write helper function to:\n",
        " update_explore_rate() - controls the balance between exploration and exploitation\n",
        " update_learning_rate() - controls how aggressively the agent updates its knowledge\n",
        "\n",
        " update_action() - uses the exploration rate to decide whether to take a\n",
        " random action (explore) or to act according to the learned Q-values (exploit)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Explore Rate Decay Function, Converges to MIN_EXPLORE_RATE\n",
        "def update_explore_rate(episode, MIN_EXPLORE_RATE):\n",
        "    # this function should return updated explore_rate\n",
        "\t# YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "# Learning Rate Decay Function, Converges to MIN_LEARNING_RATE\n",
        "def update_learning_rate(episode, MIN_LEARNING_RATE):\n",
        "    # this function should return updated learning_rate\n",
        "\t# YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "# returns an action based on current state, explore rate and Q table\n",
        "def update_action(env, state, explore_rate, Q):\n",
        "    # this function should return an action\n",
        "\t# YOUR CODE GOES HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx48WICzPkGw"
      },
      "source": [
        "### Task 3: Play with Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77QFrN9OPkGw"
      },
      "outputs": [],
      "source": [
        "# Task 3: Play with Hyperparameters\n",
        "\n",
        "# YOUR CODE GOES HERE: replace None with your values\n",
        "GAMMA_test = None\n",
        "NUM_EPISODES_test = None\n",
        "MIN_EXPLORE_RATE_test = None\n",
        "MIN_LEARNING_RATE_test = None\n",
        "STEP_SIZE_TEST = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2szdmzyT27h"
      },
      "source": [
        "## 🐤 Now use Q learning to learn the Q table\n",
        "### Task 4: Train the agent using Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJQRmWF3rFnb"
      },
      "source": [
        "The Q-value update equation, derived from Bellman equation is as follows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "Q(s, a) \\gets Q(s, a) + \\alpha*\\Big[ R(s, a) + \\gamma* \\max_{a'} Q(s', a') - Q(s, a) \\Big]\n",
        "$$"
      ],
      "metadata": {
        "id": "rrjUtRs4PwDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legend:**\n",
        "\n",
        "- **Q(s, a)**: Action-value function, estimating the expected return for taking action *a* in state *s*\n",
        "- **α**: learning rate\n",
        "- **R(s, a)**: reward received after taking action *a* in state *s*\n",
        "- **γ**: discount factor (importance of future rewards)\n",
        "- **s'**: Next state resulting from taking action *a* in state *s*  \n",
        "- **a'**: Possible actions in the next state *s'*\n",
        "- **max Q(s', a')**: Maximum estimated Q value of the next state over all possible actions *a'*\n"
      ],
      "metadata": {
        "id": "L9hrmRcSN2gQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKV3Mce-PkGx"
      },
      "outputs": [],
      "source": [
        "# Task 4: Train the agent using Q-learning\n",
        "# Suggestion: you can wrap the code to learn Q-table in a function,\n",
        "# which you can also use for Task 4 in Assignment 2\n",
        "\n",
        "# result of the training is:\n",
        "#   updated Q table (Q_test)\n",
        "#   total_reward\n",
        "#   you are free to add others, if you will\n",
        "\n",
        "tmp_env.manual_mode = False # disable printouts while training\n",
        "\n",
        "\n",
        "# initialize some variables\n",
        "total_reward = 0\n",
        "\n",
        "explore_rate = update_explore_rate(0, MIN_EXPLORE_RATE=MIN_EXPLORE_RATE_test)\n",
        "learning_rate = update_learning_rate(0, MIN_LEARNING_RATE=MIN_LEARNING_RATE_test)\n",
        "\n",
        "# YOUR CODE GOES HERE\n",
        "\n",
        "# Tip: iterate over number of episodes\n",
        "\n",
        "    # Tip: iterate over number of steps in each Episode\n",
        "    # Tip: use helper functions defined above to navigate the environment\n",
        "\n",
        "        # Tip: use the Q-learning update rule equation - Bellman equation\n",
        "\n",
        "\n",
        "\n",
        "  # Tip : print variables of interest, like episode, total_reward, explore_rate\n",
        "  # print(f\"Episode :{i}  Total_reward: {total_reward}  Explore rate: {explore_rate}\")\n",
        "  #print(\"Final Q values: \", Q_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVOVYADQPkGx"
      },
      "outputs": [],
      "source": [
        "print(f\"Total reward after training: {total_reward}, total Explore rate after training: {explore_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxlv9Qj-PkGx"
      },
      "source": [
        "## 🐥 Let's see this in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOfVD-2xPkGx"
      },
      "outputs": [],
      "source": [
        "# now we show one episode\n",
        "# yellow spot should be able to find black spot here\n",
        "\n",
        "tmp_env.manual_mode = True # enable messages\n",
        "STEP_SIZE_TEST_test = 16\n",
        "\n",
        "observation = tmp_env.reset()\n",
        "state_0 = tmp_env.get_1d_state()\n",
        "total_reward = 0\n",
        "\n",
        "for _ in range(STEP_SIZE_TEST_test):\n",
        "    tmp_env.render()\n",
        "    action = update_action(tmp_env, state_0, explore_rate, Q=Q_test)\n",
        "    obv, reward, done, info = tmp_env.step(action)\n",
        "    state_1 = tmp_env.get_1d_state()\n",
        "\n",
        "    # No need to update the Q here\n",
        "    state_0 = state_1\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Total_reward: \", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPtt6ZDoPkGx"
      },
      "outputs": [],
      "source": [
        "# the shape of Q table (Q_test)\n",
        "print(Q_test.shape)\n",
        "print(f\"Obs space dimensions :{tmp_env.observation_space.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou-VGkfDPkGx"
      },
      "outputs": [],
      "source": [
        "# print Q_table in \"observation space\" light\n",
        "Q_test.argmax(axis=1).reshape(tmp_env.observation_space.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbFTMZue12lQ"
      },
      "outputs": [],
      "source": [
        "# close the environment\n",
        "tmp_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipz9XhhQ-ZVw"
      },
      "source": [
        "# 2️⃣ Assignment 2: Trondheim treasure hunt!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKV9wMKXPkG2"
      },
      "source": [
        "# 🐔 You are ready for the Trondheim challenge now 💪\n",
        "\n",
        "*   Task 1: Initialize Q table\n",
        "*   Task 2: Write necessary helper functions for Q-learning\n",
        "*   Task 3: Play with Hyperparameters\n",
        "*   Task 4: Train the agent using Q-learning\n",
        "\n",
        "\n",
        "## **Important Note**:\n",
        "For Task 2, Task 3 and Task 4, you can (re)use the same code as in the Assignment 1.\n",
        "**These are mentioned here as the elements of the pipeline.**\n",
        "\n",
        "Most of the code from Assigment 1 is reusable, we are just using it with different environment here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ0DLZ-hPkG2"
      },
      "outputs": [],
      "source": [
        "# First and important - cleanup!\n",
        "tmp_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv_JAXXIPkG2"
      },
      "outputs": [],
      "source": [
        "# let's configure the environment and see how it looks like\n",
        "env_config = {\n",
        "    'env_image_path':'input_images/image_TRD_2.png',\n",
        "    'dead_allowed':False,\n",
        "    'remove_after_location_found':True,\n",
        "    'start_random':False,\n",
        "    'number_removable_locations':1,\n",
        "}\n",
        "\n",
        "if not os.path.exists(env_config['env_image_path']):\n",
        "  # make sure you have the folder and the image\n",
        "  print(f\"Error: The image path {env_config['env_image_path']} does not exist.\")\n",
        "else:\n",
        "  # if everything fine, we create the environment\n",
        "  tmp_env = TrondheimEnv(conf=env_config)\n",
        "  print(f\"Environment successfully created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvsn6eroPkG2"
      },
      "outputs": [],
      "source": [
        "tmp_env.reset()\n",
        "tmp_env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAC2FRZ5PkG2"
      },
      "outputs": [],
      "source": [
        "# select the action here\n",
        "my_action = 'right'\n",
        "\n",
        "# here is your action\n",
        "obs, rew, term, info = tmp_env.step(action_dict[my_action])\n",
        "print(rew)\n",
        "tmp_env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1NDKIr1PkG3"
      },
      "source": [
        "### Task 1: Initialize Q table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O94qG1KpPkG3"
      },
      "outputs": [],
      "source": [
        "# Task 1: Initialize Q table\n",
        "\"\"\"\n",
        "TODO: given the environment, initialize the Q table\n",
        "\"\"\"\n",
        "\n",
        "# Tip: pay attention to the dimensions of Q table\n",
        "# Tip: how many states the table have, how many actions\n",
        "# Extra tip:\n",
        "\"\"\"number of states is different here in comparison with first example:\n",
        "we have 2 subcases for this environment:\n",
        "when Location is there and when it is removed/destroyed\"\"\"\n",
        "\n",
        "print(f\"Observation space dimensions:  {tmp_env.observation_space.shape}\")\n",
        "print(f\"Number of possible actions: {tmp_env.action_space.n}\")\n",
        "\n",
        "# YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "Q_test = None # you need to initialize Q_test\n",
        "#print(Q_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB2XCe2ePkG3"
      },
      "source": [
        "### Task 2: Write necessary helper functions for Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEanCBRQPkG3"
      },
      "outputs": [],
      "source": [
        "# Task 2: Write necessary helper functions for Q-learning\n",
        "# Tip: just use the same helper functions you already implemented above\n",
        "\n",
        "# YOU DON'T NEED TO DO ANYTHING HERE. They are the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Caf0nsoiPkG3"
      },
      "source": [
        "### Task 3: Play with Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h52WTqg9PkG3"
      },
      "outputs": [],
      "source": [
        "# Task 3: Play with Hyperparameters\n",
        "# Tip - you can also use the same parameters as for the toy example above\n",
        "# YOUR CODE GOES HERE (set the values below)\n",
        "\n",
        "GAMMA_test = None\n",
        "NUM_EPISODES_test = None\n",
        "MIN_EXPLORE_RATE_test = None\n",
        "MIN_LEARNING_RATE_test = None\n",
        "STEP_SIZE_TEST = None\n",
        "# Depending on your parameters for the toy environment, using the same parameters here could work fine\n",
        "# if they are not working, you may want to set them to different values here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFNdPJDsPkG3"
      },
      "source": [
        "## Now use Q-learning to learn the Q table\n",
        "### Task 4: Train the agent using Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S1v2HSXPkG3"
      },
      "outputs": [],
      "source": [
        "# Task 4: Train the agent using Q-learning on\n",
        "# Tip - you can use the same function (or code) that you implemented for simple environment (Assignment 1 Task 4)\n",
        "\n",
        "# the logic is the same, just the enviroment is different\n",
        "# YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S7gcosOPkG4"
      },
      "outputs": [],
      "source": [
        "print(f\"Total reward after training: {total_reward}, total Explore rate after training: {explore_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hahegZZqPkG4"
      },
      "source": [
        "## Let's see one episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0DTHl47PkG4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#now we can show one episode\n",
        "\n",
        "tmp_env.manual_mode = True # we want to see the messages\n",
        "STEP_SIZE_TEST_test = 45\n",
        "\n",
        "image_samples = []  # Array to store images\n",
        "\n",
        "observation = tmp_env.reset()\n",
        "state_0 = tmp_env.get_1d_state()\n",
        "total_reward = 0\n",
        "\n",
        "for _ in range(STEP_SIZE_TEST_test):\n",
        "    img = tmp_env.render()\n",
        "    image_samples.append(img)\n",
        "\n",
        "    action = update_action(tmp_env, state_0, explore_rate, Q=Q_test)\n",
        "    obv, reward, done, info = tmp_env.step(action)\n",
        "    state_1 = tmp_env.get_1d_state()\n",
        "    state_0 = state_1\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Total_reward: \", total_reward)\n",
        "# self.observation.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw1c9kfbiUiw"
      },
      "source": [
        "### Simple animation of agent movement in one episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzKx98oDPkG4"
      },
      "outputs": [],
      "source": [
        "# let's make an animation of this, because it's fun\n",
        "# nothing for you to do here - just run the code\n",
        "\n",
        "import os\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "height = img.shape[0]\n",
        "width = img.shape[1]\n",
        "\n",
        "output_folder = 'output_stuff'\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "imgs = []\n",
        "for i in range(STEP_SIZE_TEST_test):\n",
        "    im = plt.imshow(image_samples[i].reshape(height, width, 3), animated=True)\n",
        "    imgs.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, imgs, interval=80, blit=True, repeat_delay=1000)\n",
        "filename_gif = os.path.join(output_folder, 'animation_test.gif')\n",
        "animate.save(filename_gif)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}